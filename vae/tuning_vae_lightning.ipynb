{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c45e2dd6-e02e-4117-963c-a6e4df39352b",
   "metadata": {},
   "source": [
    "# Tuning experiments for the Movielens dataset\n",
    "\n",
    "Feel free to re-run this and change hyperparameters as you see fit.\n",
    "\n",
    "This notebook should also be a good place to load the data only once and then train multiple times, generate charts, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2df081f7fb4bf1",
   "metadata": {},
   "source": [
    "Mode collapse: drop out regularization, l2 regularization, decaying kl divergence\n",
    "\n",
    "Regression: MSELoss"
   ]
  },
  {
   "cell_type": "code",
   "id": "4d1dc3c1-c8d8-4e91-bbc3-bf22424ab31e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T09:20:09.528654Z",
     "start_time": "2024-04-28T09:20:05.833175Z"
    }
   },
   "source": [
    "from tuning_vae_lightning import load_datasets, tune_ray"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-28 04:20:09,113\tINFO worker.py:1724 -- Started a local Ray instance.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99c8a2b7dbd1b22c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2024-04-28 04:11:13</td></tr>\n",
       "<tr><td>Running for: </td><td>00:00:32.40        </td></tr>\n",
       "<tr><td>Memory:      </td><td>29.6/31.1 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=0<br>Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None<br>Logical resource usage: 0/28 CPUs, 0/1 GPUs\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "<div class=\"messages\">\n",
       "  <h3>Messages</h3>\n",
       "  : ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
       "  \n",
       "  Number of errored trials: 4<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                                                                                                                                                </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>TorchTrainer_2f998_00000</td><td style=\"text-align: right;\">           1</td><td>/home/albert/ray_results/TorchTrainer_2024-04-28_04-10-41/TorchTrainer_2f998_00000_0_anneal_cap=0.2946,embedding_dim=128,hidden_dim=400,latent_dim=100,learning_rate=0.0042,total_anneal_ste_2024-04-28_04-10-41/error.txt</td></tr>\n",
       "<tr><td>TorchTrainer_2f998_00001</td><td style=\"text-align: right;\">           1</td><td>/home/albert/ray_results/TorchTrainer_2024-04-28_04-10-41/TorchTrainer_2f998_00001_1_anneal_cap=0.2824,embedding_dim=256,hidden_dim=800,latent_dim=100,learning_rate=0.0010,total_anneal_ste_2024-04-28_04-10-41/error.txt</td></tr>\n",
       "<tr><td>TorchTrainer_2f998_00002</td><td style=\"text-align: right;\">           1</td><td>/home/albert/ray_results/TorchTrainer_2024-04-28_04-10-41/TorchTrainer_2f998_00002_2_anneal_cap=0.1224,embedding_dim=128,hidden_dim=800,latent_dim=200,learning_rate=0.0002,total_anneal_ste_2024-04-28_04-10-41/error.txt</td></tr>\n",
       "<tr><td>TorchTrainer_2f998_00003</td><td style=\"text-align: right;\">           1</td><td>/home/albert/ray_results/TorchTrainer_2024-04-28_04-10-41/TorchTrainer_2f998_00003_3_anneal_cap=0.1124,embedding_dim=128,hidden_dim=400,latent_dim=100,learning_rate=0.0001,total_anneal_ste_2024-04-28_04-10-41/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".messages {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  padding-left: 1em;\n",
       "  overflow-y: auto;\n",
       "}\n",
       ".messages h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n",
       "\n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">         train_loop_config/an\n",
       "neal_cap</th><th style=\"text-align: right;\">    train_loop_config/em\n",
       "bedding_dim</th><th style=\"text-align: right;\">    train_loop_config/hi\n",
       "dden_dim</th><th style=\"text-align: right;\">    train_loop_config/la\n",
       "tent_dim</th><th style=\"text-align: right;\">            train_loop_config/le\n",
       "arning_rate</th><th style=\"text-align: right;\">       train_loop_config/to\n",
       "tal_anneal_steps</th><th style=\"text-align: right;\">            train_loop_config/we\n",
       "ight_decay</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>TorchTrainer_2f998_00000</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">0.29461 </td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">400</td><td style=\"text-align: right;\">100</td><td style=\"text-align: right;\">0.00416364 </td><td style=\"text-align: right;\">101450</td><td style=\"text-align: right;\">1.2156e-06 </td></tr>\n",
       "<tr><td>TorchTrainer_2f998_00001</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">0.282401</td><td style=\"text-align: right;\">256</td><td style=\"text-align: right;\">800</td><td style=\"text-align: right;\">100</td><td style=\"text-align: right;\">0.000975801</td><td style=\"text-align: right;\"> 85559</td><td style=\"text-align: right;\">8.42347e-06</td></tr>\n",
       "<tr><td>TorchTrainer_2f998_00002</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">0.122368</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">800</td><td style=\"text-align: right;\">200</td><td style=\"text-align: right;\">0.000247493</td><td style=\"text-align: right;\">124928</td><td style=\"text-align: right;\">0.000428067</td></tr>\n",
       "<tr><td>TorchTrainer_2f998_00003</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">0.112445</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">400</td><td style=\"text-align: right;\">100</td><td style=\"text-align: right;\">0.000122875</td><td style=\"text-align: right;\">139452</td><td style=\"text-align: right;\">1.17899e-05</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-28 04:10:48,663\tERROR tune_controller.py:1374 -- Trial task failed for trial TorchTrainer_2f998_00000\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/albert/.cache/pypoetry/virtualenvs/deep_learning-XCYTB7cE-py3.9/lib/python3.9/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/home/albert/.cache/pypoetry/virtualenvs/deep_learning-XCYTB7cE-py3.9/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/albert/.cache/pypoetry/virtualenvs/deep_learning-XCYTB7cE-py3.9/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/albert/.cache/pypoetry/virtualenvs/deep_learning-XCYTB7cE-py3.9/lib/python3.9/site-packages/ray/_private/worker.py\", line 2626, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 192.168.1.88, ID: 12a54b4d32af6f71de2ce182bb5b244a50eb36b8ed79a645e41bee46) where the task (task ID: ffffffffffffffffed7470a876cd92231d93681d01000000, name=_Inner.__init__, pid=524542, memory used=5.49GB) was running was 29.64GB / 31.11GB (0.952777), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c6853a2b155bdefe421264bedf3e3bba53110f07bf3bb1bf21643aaf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.88`. To see the logs of the worker, use `ray logs worker-c6853a2b155bdefe421264bedf3e3bba53110f07bf3bb1bf21643aaf*out -ip 192.168.1.88. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "522618\t11.45\t/home/albert/.cache/pypoetry/virtualenvs/deep_learning-XCYTB7cE-py3.9/bin/python -m ipykernel_launch...\n",
      "524542\t5.49\tray::_Inner.__init__\n",
      "7621\t5.04\t/home/albert/.local/share/JetBrains/Toolbox/apps/pycharm-professional/jbr/bin/java -classpath /home/...\n",
      "415126\t0.40\t/home/albert/.local/share/JetBrains/Toolbox/apps/pycharm-professional/jbr/lib/jcef_helper --type=gpu...\n",
      "399069\t0.39\t/snap/firefox/4136/usr/lib/firefox/firefox\n",
      "461618\t0.35\t/snap/firefox/4136/usr/lib/firefox/firefox -contentproc -childID 297 -isForBrowser -prefsLen 34482 -...\n",
      "462833\t0.23\t/snap/firefox/4136/usr/lib/firefox/firefox -contentproc -childID 302 -isForBrowser -prefsLen 34482 -...\n",
      "401416\t0.22\t/snap/discord/188/usr/share/discord/Discord --type=renderer --crashpad-handler-pid=401280 --enable-c...\n",
      "7896\t0.22\t./jetbrains-toolbox --minimize\n",
      "399713\t0.20\t/snap/firefox/4136/usr/lib/firefox/firefox -contentproc -childID 15 -isForBrowser -prefsLen 48046 -p...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2024-04-28 04:10:56,880\tERROR tune_controller.py:1374 -- Trial task failed for trial TorchTrainer_2f998_00001\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/albert/.cache/pypoetry/virtualenvs/deep_learning-XCYTB7cE-py3.9/lib/python3.9/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/home/albert/.cache/pypoetry/virtualenvs/deep_learning-XCYTB7cE-py3.9/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/albert/.cache/pypoetry/virtualenvs/deep_learning-XCYTB7cE-py3.9/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/albert/.cache/pypoetry/virtualenvs/deep_learning-XCYTB7cE-py3.9/lib/python3.9/site-packages/ray/_private/worker.py\", line 2626, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 192.168.1.88, ID: 12a54b4d32af6f71de2ce182bb5b244a50eb36b8ed79a645e41bee46) where the task (task ID: ffffffffffffffffb2922dabdb3032028ffd404901000000, name=_Inner.__init__, pid=524698, memory used=5.49GB) was running was 29.76GB / 31.11GB (0.956654), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 9f45dc9f7fcb20563394497ecc325a6e31b9b8ebfd95f685a09e33ba) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.88`. To see the logs of the worker, use `ray logs worker-9f45dc9f7fcb20563394497ecc325a6e31b9b8ebfd95f685a09e33ba*out -ip 192.168.1.88. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "522618\t11.45\t/home/albert/.cache/pypoetry/virtualenvs/deep_learning-XCYTB7cE-py3.9/bin/python -m ipykernel_launch...\n",
      "524698\t5.49\tray::_Inner.__init__\n",
      "7621\t5.05\t/home/albert/.local/share/JetBrains/Toolbox/apps/pycharm-professional/jbr/bin/java -classpath /home/...\n",
      "399069\t0.44\t/snap/firefox/4136/usr/lib/firefox/firefox\n",
      "415126\t0.40\t/home/albert/.local/share/JetBrains/Toolbox/apps/pycharm-professional/jbr/lib/jcef_helper --type=gpu...\n",
      "461618\t0.29\t/snap/firefox/4136/usr/lib/firefox/firefox -contentproc -childID 297 -isForBrowser -prefsLen 34482 -...\n",
      "462833\t0.23\t/snap/firefox/4136/usr/lib/firefox/firefox -contentproc -childID 302 -isForBrowser -prefsLen 34482 -...\n",
      "401416\t0.22\t/snap/discord/188/usr/share/discord/Discord --type=renderer --crashpad-handler-pid=401280 --enable-c...\n",
      "7896\t0.22\t./jetbrains-toolbox --minimize\n",
      "399713\t0.20\t/snap/firefox/4136/usr/lib/firefox/firefox -contentproc -childID 15 -isForBrowser -prefsLen 48046 -p...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2024-04-28 04:11:05,593\tERROR tune_controller.py:1374 -- Trial task failed for trial TorchTrainer_2f998_00002\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/albert/.cache/pypoetry/virtualenvs/deep_learning-XCYTB7cE-py3.9/lib/python3.9/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/home/albert/.cache/pypoetry/virtualenvs/deep_learning-XCYTB7cE-py3.9/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/albert/.cache/pypoetry/virtualenvs/deep_learning-XCYTB7cE-py3.9/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/albert/.cache/pypoetry/virtualenvs/deep_learning-XCYTB7cE-py3.9/lib/python3.9/site-packages/ray/_private/worker.py\", line 2626, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 192.168.1.88, ID: 12a54b4d32af6f71de2ce182bb5b244a50eb36b8ed79a645e41bee46) where the task (task ID: ffffffffffffffffa85ec5305ada44face36e62501000000, name=_Inner.__init__, pid=524766, memory used=5.49GB) was running was 29.68GB / 31.11GB (0.954103), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 79ce49c6fe9ac23ab2a72e42519e031792df4151255112ce89641a28) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.88`. To see the logs of the worker, use `ray logs worker-79ce49c6fe9ac23ab2a72e42519e031792df4151255112ce89641a28*out -ip 192.168.1.88. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "522618\t11.45\t/home/albert/.cache/pypoetry/virtualenvs/deep_learning-XCYTB7cE-py3.9/bin/python -m ipykernel_launch...\n",
      "524766\t5.49\tray::_Inner.__init__\n",
      "7621\t5.05\t/home/albert/.local/share/JetBrains/Toolbox/apps/pycharm-professional/jbr/bin/java -classpath /home/...\n",
      "415126\t0.41\t/home/albert/.local/share/JetBrains/Toolbox/apps/pycharm-professional/jbr/lib/jcef_helper --type=gpu...\n",
      "399069\t0.40\t/snap/firefox/4136/usr/lib/firefox/firefox\n",
      "461618\t0.28\t/snap/firefox/4136/usr/lib/firefox/firefox -contentproc -childID 297 -isForBrowser -prefsLen 34482 -...\n",
      "462833\t0.23\t/snap/firefox/4136/usr/lib/firefox/firefox -contentproc -childID 302 -isForBrowser -prefsLen 34482 -...\n",
      "401416\t0.22\t/snap/discord/188/usr/share/discord/Discord --type=renderer --crashpad-handler-pid=401280 --enable-c...\n",
      "7896\t0.22\t./jetbrains-toolbox --minimize\n",
      "399713\t0.20\t/snap/firefox/4136/usr/lib/firefox/firefox -contentproc -childID 15 -isForBrowser -prefsLen 48046 -p...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2024-04-28 04:11:13,553\tERROR tune_controller.py:1374 -- Trial task failed for trial TorchTrainer_2f998_00003\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/albert/.cache/pypoetry/virtualenvs/deep_learning-XCYTB7cE-py3.9/lib/python3.9/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/home/albert/.cache/pypoetry/virtualenvs/deep_learning-XCYTB7cE-py3.9/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/albert/.cache/pypoetry/virtualenvs/deep_learning-XCYTB7cE-py3.9/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/albert/.cache/pypoetry/virtualenvs/deep_learning-XCYTB7cE-py3.9/lib/python3.9/site-packages/ray/_private/worker.py\", line 2626, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 192.168.1.88, ID: 12a54b4d32af6f71de2ce182bb5b244a50eb36b8ed79a645e41bee46) where the task (task ID: ffffffffffffffff048dc5197bc3efd658f65b5d01000000, name=_Inner.__init__, pid=524835, memory used=5.49GB) was running was 29.65GB / 31.11GB (0.953044), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 1cc56b7a10b50aef5cf5ca4c6246d82f04d8d46b1e959b4fa6bf6501) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.1.88`. To see the logs of the worker, use `ray logs worker-1cc56b7a10b50aef5cf5ca4c6246d82f04d8d46b1e959b4fa6bf6501*out -ip 192.168.1.88. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "522618\t11.45\t/home/albert/.cache/pypoetry/virtualenvs/deep_learning-XCYTB7cE-py3.9/bin/python -m ipykernel_launch...\n",
      "524835\t5.49\tray::_Inner.__init__\n",
      "7621\t5.05\t/home/albert/.local/share/JetBrains/Toolbox/apps/pycharm-professional/jbr/bin/java -classpath /home/...\n",
      "399069\t0.44\t/snap/firefox/4136/usr/lib/firefox/firefox\n",
      "415126\t0.41\t/home/albert/.local/share/JetBrains/Toolbox/apps/pycharm-professional/jbr/lib/jcef_helper --type=gpu...\n",
      "461618\t0.28\t/snap/firefox/4136/usr/lib/firefox/firefox -contentproc -childID 297 -isForBrowser -prefsLen 34482 -...\n",
      "462833\t0.23\t/snap/firefox/4136/usr/lib/firefox/firefox -contentproc -childID 302 -isForBrowser -prefsLen 34482 -...\n",
      "401416\t0.22\t/snap/discord/188/usr/share/discord/Discord --type=renderer --crashpad-handler-pid=401280 --enable-c...\n",
      "7896\t0.22\t./jetbrains-toolbox --minimize\n",
      "399713\t0.20\t/snap/firefox/4136/usr/lib/firefox/firefox -contentproc -childID 15 -isForBrowser -prefsLen 48046 -p...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2024-04-28 04:11:13,556\tERROR tune.py:1038 -- Trials did not complete: [TorchTrainer_2f998_00000, TorchTrainer_2f998_00001, TorchTrainer_2f998_00002, TorchTrainer_2f998_00003]\n",
      "2024-04-28 04:11:13,557\tINFO tune.py:1042 -- Total run time: 32.42 seconds (32.40 seconds for the tuning loop).\n",
      "2024-04-28 04:11:13,558\tWARNING experiment_analysis.py:193 -- Failed to fetch metrics for 4 trial(s):\n",
      "- TorchTrainer_2f998_00000: FileNotFoundError('Could not fetch metrics for TorchTrainer_2f998_00000: both result.json and progress.csv were not found at /home/albert/ray_results/TorchTrainer_2024-04-28_04-10-41/TorchTrainer_2f998_00000_0_anneal_cap=0.2946,embedding_dim=128,hidden_dim=400,latent_dim=100,learning_rate=0.0042,total_anneal_ste_2024-04-28_04-10-41')\n",
      "- TorchTrainer_2f998_00001: FileNotFoundError('Could not fetch metrics for TorchTrainer_2f998_00001: both result.json and progress.csv were not found at /home/albert/ray_results/TorchTrainer_2024-04-28_04-10-41/TorchTrainer_2f998_00001_1_anneal_cap=0.2824,embedding_dim=256,hidden_dim=800,latent_dim=100,learning_rate=0.0010,total_anneal_ste_2024-04-28_04-10-41')\n",
      "- TorchTrainer_2f998_00002: FileNotFoundError('Could not fetch metrics for TorchTrainer_2f998_00002: both result.json and progress.csv were not found at /home/albert/ray_results/TorchTrainer_2024-04-28_04-10-41/TorchTrainer_2f998_00002_2_anneal_cap=0.1224,embedding_dim=128,hidden_dim=800,latent_dim=200,learning_rate=0.0002,total_anneal_ste_2024-04-28_04-10-41')\n",
      "- TorchTrainer_2f998_00003: FileNotFoundError('Could not fetch metrics for TorchTrainer_2f998_00003: both result.json and progress.csv were not found at /home/albert/ray_results/TorchTrainer_2024-04-28_04-10-41/TorchTrainer_2f998_00003_3_anneal_cap=0.1124,embedding_dim=128,hidden_dim=400,latent_dim=100,learning_rate=0.0001,total_anneal_ste_2024-04-28_04-10-41')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResultGrid<[\n",
       "  Result(\n",
       "    error='TuneError',\n",
       "    metrics={},\n",
       "    path='/home/albert/ray_results/TorchTrainer_2024-04-28_04-10-41/TorchTrainer_2f998_00000_0_anneal_cap=0.2946,embedding_dim=128,hidden_dim=400,latent_dim=100,learning_rate=0.0042,total_anneal_ste_2024-04-28_04-10-41',\n",
       "    filesystem='local',\n",
       "    checkpoint=None\n",
       "  ),\n",
       "  Result(\n",
       "    error='TuneError',\n",
       "    metrics={},\n",
       "    path='/home/albert/ray_results/TorchTrainer_2024-04-28_04-10-41/TorchTrainer_2f998_00001_1_anneal_cap=0.2824,embedding_dim=256,hidden_dim=800,latent_dim=100,learning_rate=0.0010,total_anneal_ste_2024-04-28_04-10-41',\n",
       "    filesystem='local',\n",
       "    checkpoint=None\n",
       "  ),\n",
       "  Result(\n",
       "    error='TuneError',\n",
       "    metrics={},\n",
       "    path='/home/albert/ray_results/TorchTrainer_2024-04-28_04-10-41/TorchTrainer_2f998_00002_2_anneal_cap=0.1224,embedding_dim=128,hidden_dim=800,latent_dim=200,learning_rate=0.0002,total_anneal_ste_2024-04-28_04-10-41',\n",
       "    filesystem='local',\n",
       "    checkpoint=None\n",
       "  ),\n",
       "  Result(\n",
       "    error='TuneError',\n",
       "    metrics={},\n",
       "    path='/home/albert/ray_results/TorchTrainer_2024-04-28_04-10-41/TorchTrainer_2f998_00003_3_anneal_cap=0.1124,embedding_dim=128,hidden_dim=400,latent_dim=100,learning_rate=0.0001,total_anneal_ste_2024-04-28_04-10-41',\n",
       "    filesystem='local',\n",
       "    checkpoint=None\n",
       "  )\n",
       "]>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tune_ray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2682bda2-cddf-445c-9843-75c32056be53",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T08:48:28.447016Z",
     "start_time": "2024-04-28T08:48:28.445625Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
